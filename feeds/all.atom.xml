<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Pythran stories</title><link href="http://serge-sans-paille.github.io/pythran-stories/" rel="alternate"></link><link href="http://serge-sans-paille.github.io/pythran-stories/feeds/all.atom.xml" rel="self"></link><id>http://serge-sans-paille.github.io/pythran-stories/</id><updated>2016-04-05T00:00:00+02:00</updated><entry><title>Micro-Benchmarking Julia, C++ and Pythran on an Economics kernel</title><link href="http://serge-sans-paille.github.io/pythran-stories/micro-benchmarking-julia-c-and-pythran-on-an-economics-kernel.html" rel="alternate"></link><updated>2016-04-05T00:00:00+02:00</updated><author><name>Lightjohn</name></author><id>tag:serge-sans-paille.github.io,2016-04-05:pythran-stories/micro-benchmarking-julia-c-and-pythran-on-an-economics-kernel.html</id><summary type="html">&lt;div class="section" id="the-benchmark"&gt;
&lt;h2&gt;The benchmark&lt;/h2&gt;
&lt;p&gt;First there was a paper &lt;a class="footnote-reference" href="#id4" id="id1"&gt;[0]&lt;/a&gt;,
in the paper there was a github &lt;a class="footnote-reference" href="#id5" id="id2"&gt;[1]&lt;/a&gt; and
in the github &lt;a class="footnote-reference" href="#id5" id="id3"&gt;[1]&lt;/a&gt; some benchmarks.
In my case I wanted to re-run the Julia code because the language is changing quickly and so may run better now.
But the day before I discovered Pythran so why not test both?&lt;/p&gt;
&lt;p&gt;And so let's re-run two benchmarks: &lt;a class="reference external" href="https://github.com/jesusfv/Comparison-Programming-Languages-Economics/blob/master/RBC_CPP.cpp"&gt;C++&lt;/a&gt; , &lt;a class="reference external" href="https://github.com/jesusfv/Comparison-Programming-Languages-Economics/blob/master/RBC_Julia.jl"&gt;Julia&lt;/a&gt; and add a new one, Pythran.&lt;/p&gt;
&lt;div class="section" id="c-results"&gt;
&lt;h3&gt;C++ results&lt;/h3&gt;
&lt;p&gt;Compiling and running the C++ code was easy:&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
% g++ -O3 RBC_CPP.cpp -o testcpp
&lt;/pre&gt;
&lt;p&gt;then&lt;/p&gt;
&lt;pre class="code julia literal-block"&gt;
&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="o"&gt;./&lt;/span&gt;&lt;span class="n"&gt;testcpp&lt;/span&gt;
&lt;span class="n"&gt;Output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.562731&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Capital&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.178198&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Consumption&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.384533&lt;/span&gt;
&lt;span class="n"&gt;Iteration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Sup&lt;/span&gt; &lt;span class="n"&gt;Diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0527416&lt;/span&gt;
&lt;span class="n"&gt;Iteration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Sup&lt;/span&gt; &lt;span class="n"&gt;Diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0313469&lt;/span&gt;
&lt;span class="n"&gt;Iteration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Sup&lt;/span&gt; &lt;span class="n"&gt;Diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0187035&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;Iteration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;230&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Sup&lt;/span&gt; &lt;span class="n"&gt;Diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;3.87636e-07&lt;/span&gt;
&lt;span class="n"&gt;Iteration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;240&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Sup&lt;/span&gt; &lt;span class="n"&gt;Diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;2.32197e-07&lt;/span&gt;
&lt;span class="n"&gt;Iteration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;250&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Sup&lt;/span&gt; &lt;span class="n"&gt;Diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.39087e-07&lt;/span&gt;
&lt;span class="n"&gt;Iteration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;257&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Sup&lt;/span&gt; &lt;span class="n"&gt;Diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;9.71604e-08&lt;/span&gt;

&lt;span class="n"&gt;My&lt;/span&gt; &lt;span class="n"&gt;check&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.146549&lt;/span&gt;
&lt;span class="n"&gt;Elapsed&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt; &lt;span class="nb"&gt;is&lt;/span&gt;   &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;2.40271&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="julia-results"&gt;
&lt;h3&gt;Julia results&lt;/h3&gt;
&lt;p&gt;The Julia code:&lt;/p&gt;
&lt;p&gt;we run &lt;cite&gt;julia&lt;/cite&gt;:&lt;/p&gt;
&lt;pre class="code julia literal-block"&gt;
&lt;span class="n"&gt;include&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;RBC_Julia.jl&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;julia&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;&amp;#64;&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt; &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;Output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.5627314338711378&lt;/span&gt; &lt;span class="n"&gt;Capital&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.178198287392527&lt;/span&gt; &lt;span class="n"&gt;Consumption&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.3845331464786108&lt;/span&gt;
&lt;span class="n"&gt;Iteration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;Sup&lt;/span&gt; &lt;span class="n"&gt;Diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.05274159340733661&lt;/span&gt;
&lt;span class="n"&gt;Iteration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="n"&gt;Sup&lt;/span&gt; &lt;span class="n"&gt;Diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.031346949265852075&lt;/span&gt;
&lt;span class="n"&gt;Iteration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt; &lt;span class="n"&gt;Sup&lt;/span&gt; &lt;span class="n"&gt;Diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.01870345989335709&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;Iteration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;230&lt;/span&gt; &lt;span class="n"&gt;Sup&lt;/span&gt; &lt;span class="n"&gt;Diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;3.876361940324813e-7&lt;/span&gt;
&lt;span class="n"&gt;Iteration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;240&lt;/span&gt; &lt;span class="n"&gt;Sup&lt;/span&gt; &lt;span class="n"&gt;Diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;2.3219657929729465e-7&lt;/span&gt;
&lt;span class="n"&gt;Iteration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;250&lt;/span&gt; &lt;span class="n"&gt;Sup&lt;/span&gt; &lt;span class="n"&gt;Diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.3908720952748865e-7&lt;/span&gt;
&lt;span class="n"&gt;Iteration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;257&lt;/span&gt; &lt;span class="n"&gt;Sup&lt;/span&gt; &lt;span class="n"&gt;Diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;9.716035642703957e-8&lt;/span&gt;

&lt;span class="n"&gt;My&lt;/span&gt; &lt;span class="n"&gt;check&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.1465491436962635&lt;/span&gt;
&lt;span class="mf"&gt;3.001183&lt;/span&gt; &lt;span class="n"&gt;seconds&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;3.84&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="n"&gt;allocations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;703.276&lt;/span&gt; &lt;span class="n"&gt;MB&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.68&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;gc&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Not bad!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="python-pythran-and-numba"&gt;
&lt;h3&gt;Python: Pythran  and Numba&lt;/h3&gt;
&lt;p&gt;Now some pythran code, we use the Numba version as starter: so we remove the Numba decorator and
replace it by a Pythran comment:&lt;/p&gt;
&lt;pre class="code python literal-block"&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numba&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;autojit&lt;/span&gt;

&lt;span class="nd"&gt;&amp;#64;autojit&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;innerloop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bbeta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nGridCapital&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gridCapitalNextPeriod&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mOutput&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nProductivity&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vGridCapital&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;expectedValueFunction&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mValueFunction&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mValueFunctionNew&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mPolicyFunction&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;to&lt;/p&gt;
&lt;pre class="code python literal-block"&gt;
&lt;span class="c"&gt;#pythran export innerloop(float, int, int, float[][], int, float[], float[][], float[][], float[][], float[][])&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;innerloop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bbeta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nGridCapital&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gridCapitalNextPeriod&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mOutput&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nProductivity&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vGridCapital&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;expectedValueFunction&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mValueFunction&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mValueFunctionNew&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mPolicyFunction&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Easy? not quite... while pythranisation of the code, something went wrong, but
no idea why! With some (many) help, the solution was found: the idea was to extract the innerloop into a
new file and run Pythran on it then calling it from the main code.&lt;/p&gt;
&lt;p&gt;The function is in &lt;tt class="docutils literal"&gt;je.py&lt;/tt&gt; and the main code is &lt;tt class="docutils literal"&gt;run_je.py&lt;/tt&gt;&lt;/p&gt;
&lt;p&gt;Let's run the code:&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
% &lt;span class="nb"&gt;time &lt;/span&gt;python2 run_je.py
&lt;span class="nv"&gt;Output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  0.562731433871  &lt;span class="nv"&gt;Capital&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  0.178198287393  &lt;span class="nv"&gt;Consumption&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  0.384533146479
&lt;span class="nv"&gt;Iteration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="m"&gt;1&lt;/span&gt; , Sup &lt;span class="nv"&gt;Diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  0.0527415934073
&lt;span class="nv"&gt;Iteration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="m"&gt;10&lt;/span&gt; , Sup &lt;span class="nv"&gt;Diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  0.0313469492659
&lt;span class="nv"&gt;Iteration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="m"&gt;20&lt;/span&gt; , Sup &lt;span class="nv"&gt;Diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  0.0187034598934
&lt;span class="o"&gt;[&lt;/span&gt;...&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="nv"&gt;Iteration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="m"&gt;230&lt;/span&gt; , Sup &lt;span class="nv"&gt;Diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  3.87636194032e-07
&lt;span class="nv"&gt;Iteration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="m"&gt;240&lt;/span&gt; , Sup &lt;span class="nv"&gt;Diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  2.32196579297e-07
&lt;span class="nv"&gt;Iteration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="m"&gt;250&lt;/span&gt; , Sup &lt;span class="nv"&gt;Diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  1.39087209527e-07
python2 run_je.py  2,45s user 0,08s system 94% cpu 2,666 total
&lt;/pre&gt;
&lt;p&gt;And it is very nice!&lt;/p&gt;
&lt;p&gt;And just for fun, the &lt;a class="reference external" href="https://github.com/jesusfv/Comparison-Programming-Languages-Economics/blob/master/RBC_Python_Numba.py"&gt;Numba version&lt;/a&gt;:&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
% &lt;span class="nb"&gt;time &lt;/span&gt;python2 RBC_Python_Numba.py
&lt;span class="nv"&gt;Output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  0.562731433871  &lt;span class="nv"&gt;Capital&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  0.178198287393  &lt;span class="nv"&gt;Consumption&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  0.384533146479
&lt;span class="nv"&gt;Iteration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="m"&gt;1&lt;/span&gt; , Sup &lt;span class="nv"&gt;Diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  0.0527415934073
&lt;span class="nv"&gt;Iteration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="m"&gt;10&lt;/span&gt; , Sup &lt;span class="nv"&gt;Diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  0.0313469492659
&lt;span class="nv"&gt;Iteration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="m"&gt;20&lt;/span&gt; , Sup &lt;span class="nv"&gt;Diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  0.0187034598934
&lt;span class="o"&gt;[&lt;/span&gt;...&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="nv"&gt;Iteration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="m"&gt;230&lt;/span&gt; , Sup &lt;span class="nv"&gt;Diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  3.87636194032e-07
&lt;span class="nv"&gt;Iteration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="m"&gt;240&lt;/span&gt; , Sup &lt;span class="nv"&gt;Diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  2.32196579297e-07
&lt;span class="nv"&gt;Iteration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="m"&gt;250&lt;/span&gt; , Sup &lt;span class="nv"&gt;Diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  1.39087209527e-07
&lt;span class="nv"&gt;Iteration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="m"&gt;257&lt;/span&gt; , Sup &lt;span class="nv"&gt;Duff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  9.71603566491e-08

My &lt;span class="nv"&gt;Check&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  0.146549143696
Elapse &lt;span class="nb"&gt;time&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; is  3.00302290916
&lt;/pre&gt;
&lt;p&gt;So in the end we have:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="50%" /&gt;
&lt;col width="50%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Language&lt;/th&gt;
&lt;th class="head"&gt;Time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;C++&lt;/td&gt;
&lt;td&gt;2.4 sec&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Pythran&lt;/td&gt;
&lt;td&gt;2.4 sec&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Numba&lt;/td&gt;
&lt;td&gt;3.0 sec&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Julia&lt;/td&gt;
&lt;td&gt;3.0 sec&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;These benchs were run on a modest Pentium R 3550M &amp;#64; 2.3GHz&lt;/p&gt;
&lt;p&gt;But what amaze me was the fact that with Pythran we were able to my high-end Intel i7 machine.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;To conclude, Pythran is for me still young, like Julia, but for a little cost and no particular knowlegde you can
get the same performances as C code in Python. It worth the time to take a look to Pythran.&lt;/p&gt;
&lt;p&gt;So good luck Pythran!&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="id4" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id1"&gt;[0]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a class="reference external" href="http://economics.sas.upenn.edu/~jesusfv/comparison_languages.pdf"&gt;http://economics.sas.upenn.edu/~jesusfv/comparison_languages.pdf&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id5" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[1]&lt;/td&gt;&lt;td&gt;&lt;em&gt;(&lt;a class="fn-backref" href="#id2"&gt;1&lt;/a&gt;, &lt;a class="fn-backref" href="#id3"&gt;2&lt;/a&gt;)&lt;/em&gt; &lt;a class="reference external" href="https://github.com/jesusfv/Comparison-Programming-Languages-Economics"&gt;https://github.com/jesusfv/Comparison-Programming-Languages-Economics&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
</summary></entry><entry><title>Compiler Flags</title><link href="http://serge-sans-paille.github.io/pythran-stories/compiler-flags.html" rel="alternate"></link><updated>2016-03-29T00:00:00+02:00</updated><author><name>serge-sans-paille</name></author><id>tag:serge-sans-paille.github.io,2016-03-29:pythran-stories/compiler-flags.html</id><summary type="html">&lt;div class="section" id="when-size-matters"&gt;
&lt;h2&gt;When Size Matters&lt;/h2&gt;
&lt;p&gt;Everything started a few days ago with a Pythran user complaining about the
size of the binaries generated by Pythran. In essence, take the following code
&lt;cite&gt;cda.py&lt;/cite&gt;:&lt;/p&gt;
&lt;pre class="code python literal-block"&gt;
&lt;span class="c"&gt;#pythran export closest_distance_arrays(float, float, float[], float[])&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;math&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;closest_distance_arrays&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lat1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;long1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;latitudes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;longitudes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;degrees_to_radians&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mf"&gt;180.0&lt;/span&gt;
    &lt;span class="n"&gt;phi1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;90.0&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;lat1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;degrees_to_radians&lt;/span&gt;
    &lt;span class="n"&gt;phi2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;90.0&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;latitudes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;degrees_to_radians&lt;/span&gt;
    &lt;span class="n"&gt;theta1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;long1&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;degrees_to_radians&lt;/span&gt;
    &lt;span class="n"&gt;theta2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;longitudes&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;degrees_to_radians&lt;/span&gt;
    &lt;span class="n"&gt;cos&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theta1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;theta2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
           &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;arc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arccos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;cos&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arc&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;arc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;It doesn't even weight a kilobyte, and when benchmarked, it runs in a few milliseconds:&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
&amp;gt; python -m timeit -s &lt;span class="s1"&gt;'import numpy as np; n = 20000 ; lat, lon = np.random.rand(n), np.random.rand(n); x,y = np.random.rand(), np.random.rand(); from cda import closest_distance_arrays'&lt;/span&gt; &lt;span class="s1"&gt;'closest_distance_arrays(x,y,lat, lon)'&lt;/span&gt;
&lt;span class="m"&gt;100&lt;/span&gt; loops, best of 3: 1.95 msec per loop
&lt;/pre&gt;
&lt;p&gt;Thanks to the &lt;tt class="docutils literal"&gt;#pythran export&lt;/tt&gt; annotation, Pythran can turn it into a native
library that runs slightly faster than the Python version:&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
&amp;gt; pythran cda.py
&amp;gt; python -m timeit -s &lt;span class="s1"&gt;'import numpy as np; n = 20000 ; lat, lon = np.random.rand(n), np.random.rand(n); x,y = np.random.rand(), np.random.rand(); from cda import closest_distance_arrays'&lt;/span&gt; &lt;span class="s1"&gt;'closest_distance_arrays(x,y,lat, lon)'&lt;/span&gt;
&lt;span class="m"&gt;1000&lt;/span&gt; loops, best of 3: 1.17 msec per loop
&lt;/pre&gt;
&lt;p&gt;It is, however, a very big binary:&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
&amp;gt; ls -lh cda.so
-rwxr-xr-x &lt;span class="m"&gt;1&lt;/span&gt; sguelton sguelton 1.3M Mar &lt;span class="m"&gt;29&lt;/span&gt; 18:10 cda.so*
&lt;/pre&gt;
&lt;p&gt;Who wants to multiply the binary size by &lt;tt class="docutils literal"&gt;2e3&lt;/tt&gt; to get less than a &lt;tt class="docutils literal"&gt;x2&lt;/tt&gt; speedup?&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-culprits-debug-informations"&gt;
&lt;h2&gt;The culprits: Debug Informations&lt;/h2&gt;
&lt;p&gt;One can call Pythran with the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-v&lt;/span&gt;&lt;/tt&gt; flag to inspect part of its internal,
especially the C++ compiler call done to perform object code generation and
linking:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;
&amp;gt; pythran cda.py -v
running build_ext
running build_src
build_src
building extension &amp;quot;cda&amp;quot; sources
build_src: building npy-pkg config files
new_compiler returns distutils.unixccompiler.UnixCCompiler
INFO     customize UnixCCompiler
customize UnixCCompiler using build_ext
********************************************************************************
distutils.unixccompiler.UnixCCompiler
linker_exe    = ['gcc']
compiler_so   = ['gcc', '-DNDEBUG', '-g', '-fwrapv', '-O2', '-Wall', '-Wstrict-prototypes', '-fno-strict-aliasing', '-g', '-O2', '-fPIC']
archiver      = ['x86_64-linux-gnu-gcc-ar', 'rc']
preprocessor  = ['gcc', '-E']
linker_so     = ['x86_64-linux-gnu-gcc', '-pthread', '-shared', '-Wl,-O1', '-Wl,-Bsymbolic-functions', '-Wl,-z,relro', '-fno-strict-aliasing', '-DNDEBUG', '-g', '-fwrapv', '-O2', '-Wall', '-Wstrict-prototypes', '-Wdate-time', '-D_FORTIFY_SOURCE=2', '-g', '-fstack-protector-strong', '-Wformat', '-Werror=format-security', '-Wl,-z,relro', '-g', '-O2']
compiler_cxx  = ['g++']
ranlib        = None
compiler      = ['gcc', '-DNDEBUG', '-g', '-fwrapv', '-O2', '-Wall', '-Wstrict-prototypes', '-fno-strict-aliasing', '-g', '-O2']
libraries     = []
library_dirs  = []
include_dirs  = ['/usr/include/python2.7']
[...]
INFO     Generated module: cda
INFO     Output: /home/sguelton/sources/pythran/cda.so
&lt;/pre&gt;
&lt;p&gt;That's a pretty long trace, but that's what verbose mode is for. The
enlightened reader noticed that we use &lt;tt class="docutils literal"&gt;distutils&lt;/tt&gt; under the hood to abstract
the compiler calls, and that's why we're getting some funky compiler flags like
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-g&lt;/span&gt; &lt;span class="pre"&gt;-fwrapv&lt;/span&gt; &lt;span class="pre"&gt;-O2&lt;/span&gt; &lt;span class="pre"&gt;-Wall&lt;/span&gt; &lt;span class="pre"&gt;-fno-strict-aliasing&lt;/span&gt; &lt;span class="pre"&gt;-g&lt;/span&gt; &lt;span class="pre"&gt;-O2&lt;/span&gt; &lt;span class="pre"&gt;-fPIC&lt;/span&gt;&lt;/tt&gt; or even funkier
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-fstack-protector-strong&lt;/span&gt; &lt;span class="pre"&gt;-Wformat&lt;/span&gt; &lt;span class="pre"&gt;-Werror=format-security&lt;/span&gt; &lt;span class="pre"&gt;-Wl,-z,relro&lt;/span&gt;&lt;/tt&gt;.
That's the default for native python extensions on my distrib. Funny enough the
last ones are hardening flags used to improve the security of the binary and I
wrote a (passionating) article about it for Quarkslab &lt;a class="footnote-reference" href="#id4" id="id1"&gt;[0]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It turns out &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-g&lt;/span&gt;&lt;/tt&gt; (and C++) is responsible for the fat binary: if we simply
strip the binary, we get back to a decent size:&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
&amp;gt; strip cda.so
&amp;gt; ls -lh cda.so
-rwxr-xr-x &lt;span class="m"&gt;1&lt;/span&gt; sguelton sguelton 151K Mar &lt;span class="m"&gt;29&lt;/span&gt; 18:26 cda.so
&lt;/pre&gt;
&lt;p&gt;As Pythran users generally don't want the debug info on the generated native
code, we chose to strip them by default, using the linker flag
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-Wl,-strip-all&lt;/span&gt;&lt;/tt&gt; that removes all symbol informations, including debug
symbols.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="a-step-further-default-symbol-visibility"&gt;
&lt;h2&gt;A Step further: Default Symbol visibility&lt;/h2&gt;
&lt;p&gt;While we're at it, let's call &lt;tt class="docutils literal"&gt;nm&lt;/tt&gt; to check if any symbol remains in the
binary. After all, the Python interpreter still needs some of them to load the
native extension!&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
&amp;gt; nm -C -D cda.so
&lt;span class="o"&gt;[&lt;/span&gt;...&lt;span class="o"&gt;]&lt;/span&gt; skipping &amp;gt; &lt;span class="m"&gt;900&lt;/span&gt; entries
000000000001ed00 u nt2::ext::implement&amp;lt;nt2::tag::rem_pio2_ &lt;span class="o"&gt;(&lt;/span&gt;boost::dispatch::meta::scalar_&amp;lt;boost::dispatch::meta::double_&amp;lt;double&amp;gt; &amp;gt;, boost::dispatch::meta::scalar_&amp;lt;boost::dispatch::meta::double_&amp;lt;double&amp;gt; &amp;gt;, boost::dispatch::meta::scalar_&amp;lt;boost::dispatch::meta::double_&amp;lt;double&amp;gt; &amp;gt;&lt;span class="o"&gt;)&lt;/span&gt;, boost::dispatch::tag::cpu_, void&amp;gt;::__kernel_rem_pio2&lt;span class="o"&gt;(&lt;/span&gt;double*, double*, int, int, int, int const*&lt;span class="o"&gt;)&lt;/span&gt;::PIo2
000000000001edc0 u nt2::ext::implement&amp;lt;nt2::tag::rem_pio2_ &lt;span class="o"&gt;(&lt;/span&gt;boost::dispatch::meta::scalar_&amp;lt;boost::dispatch::meta::double_&amp;lt;double&amp;gt; &amp;gt;, boost::dispatch::meta::scalar_&amp;lt;boost::dispatch::meta::double_&amp;lt;double&amp;gt; &amp;gt;, boost::dispatch::meta::scalar_&amp;lt;boost::dispatch::meta::double_&amp;lt;double&amp;gt; &amp;gt;&lt;span class="o"&gt;)&lt;/span&gt;, boost::dispatch::tag::cpu_, void&amp;gt;::__ieee754_rem_pio2&lt;span class="o"&gt;(&lt;/span&gt;double, double*&lt;span class="o"&gt;)&lt;/span&gt;::two_over_pi
000000000001ed40 u nt2::ext::implement&amp;lt;nt2::tag::rem_pio2_ &lt;span class="o"&gt;(&lt;/span&gt;boost::dispatch::meta::scalar_&amp;lt;boost::dispatch::meta::double_&amp;lt;double&amp;gt; &amp;gt;, boost::dispatch::meta::scalar_&amp;lt;boost::dispatch::meta::double_&amp;lt;double&amp;gt; &amp;gt;, boost::dispatch::meta::scalar_&amp;lt;boost::dispatch::meta::double_&amp;lt;double&amp;gt; &amp;gt;&lt;span class="o"&gt;)&lt;/span&gt;, boost::dispatch::tag::cpu_, void&amp;gt;::__ieee754_rem_pio2&lt;span class="o"&gt;(&lt;/span&gt;double, double*&lt;span class="o"&gt;)&lt;/span&gt;::npio2_hw
&lt;/pre&gt;
&lt;p&gt;I can tell you Python is &lt;em&gt;not&lt;/em&gt; using nt2 dispatch mechanism to load native
extensions. Again, the default compiler settings are responsible for this
noise, and the relevant compiler flag is &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-fvisibility=hidden&lt;/span&gt;&lt;/tt&gt; that tells the
compiler than only the functions flagged with a special attribute are part of
the external ABI, the other ones are not exported. As Python uses a single
entry point to load Pythran modules, namely &lt;tt class="docutils literal"&gt;PyInit_cda&lt;/tt&gt; for Python3 modules
and &lt;tt class="docutils literal"&gt;initcda&lt;/tt&gt; for Python2 modules &lt;a class="footnote-reference" href="#id5" id="id2"&gt;[1]&lt;/a&gt;, one can add the &lt;tt class="docutils literal"&gt;__attribute__
&lt;span class="pre"&gt;((visibility(&amp;quot;default&amp;quot;)))&lt;/span&gt;&lt;/tt&gt; on this symbol and it will be the only exported
one. This slightly impacts the code size, may decrease loading time and
eventually gives the compiler more optimization opportunities, but nothing
significant there (131K), apart the pleasure of generating cleaner binaries.
That's also going to be the default for next Pythran version.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="out-of-chance-getting-faster-binaries"&gt;
&lt;h2&gt;Out of chance: getting faster binaries&lt;/h2&gt;
&lt;p&gt;In the (huge) info pages of GCC, near the doc of &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-fvisibility=hidden&lt;/span&gt;&lt;/tt&gt;,
there's this (GCC only) compiler flag, &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-fwhole-program&lt;/span&gt;&lt;/tt&gt; that implements some
kind of Link Time Optimization, in the sense that it tells the compiler to
consider the current compilation unit (or code) as a whole program. As
specified in the GCC man page, &amp;quot;All public functions and variables with the
exception of &amp;quot;main&amp;quot; and those merged by attribute &amp;quot;externally_visible&amp;quot; become
static functions and in effect are optimized more aggressively by
interprocedural optimizers.&amp;quot;, which basically means that every function is
considered static except for &amp;quot;main&amp;quot; and the ones that are explicitly told not
to be.  This allows the compiler for instance to remove functions that are
always inlined, and thus win space. So we flag the &lt;tt class="docutils literal"&gt;initcda&lt;/tt&gt; function with
&lt;tt class="docutils literal"&gt;__attribute__ ((externally_visible))&lt;/tt&gt;. That sounds a bit redundant to me
with the visibility attribute, but it turns out this triggers abunch of
different optimization path that gives us a significantly smaller binary, that
runs slightly faster:&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
&amp;gt; pythran cda.py -fvisibility&lt;span class="o"&gt;=&lt;/span&gt;hidden -fwhole-program -Wl,-strip-all
&amp;gt; ls -lh cda.so
-rwxr-xr-x &lt;span class="m"&gt;1&lt;/span&gt; sguelton sguelton 31K Mar &lt;span class="m"&gt;29&lt;/span&gt; 18:52 cda.so*
&amp;gt; python -m timeit -s &lt;span class="s1"&gt;'import numpy as np; n = 20000 ; lat, lon = np.random.rand(n), np.random.rand(n); x,y = np.random.rand(), np.random.rand(); from cda import closest_distance_arrays'&lt;/span&gt; &lt;span class="s1"&gt;'closest_distance_arrays(x,y,lat, lon)'&lt;/span&gt;
&lt;span class="m"&gt;1000&lt;/span&gt; loops, best of 3: 1.15 msec per loop
&lt;/pre&gt;
&lt;p&gt;All these flags are now the default on Linux.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="playing-with-the-optimization-flags-too"&gt;
&lt;h2&gt;Playing with the optimization flags too&lt;/h2&gt;
&lt;p&gt;The default optimization flag is &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-O2&lt;/span&gt;&lt;/tt&gt;, and that's generally a decent choice.
On &lt;tt class="docutils literal"&gt;cda.py&lt;/tt&gt;, using &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-O3&lt;/span&gt;&lt;/tt&gt; does not give much change (gcc 4.9):&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
&amp;gt; pythran cda.py -fvisibility&lt;span class="o"&gt;=&lt;/span&gt;hidden -fwhole-program -Wl,-strip-all -O3
&amp;gt; python -m timeit &lt;span class="o"&gt;[&lt;/span&gt;...&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="m"&gt;1000&lt;/span&gt; loops, best of 3: 1.14 msec per loop
&lt;/pre&gt;
&lt;p&gt;Asking for code specific to my CPU using &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-march=native&lt;/span&gt;&lt;/tt&gt; actually gives some improvments&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
&amp;gt; pythran cda.py -fvisibility&lt;span class="o"&gt;=&lt;/span&gt;hidden -fwhole-program -Wl,-strip-all -O3 -march&lt;span class="o"&gt;=&lt;/span&gt;native
&amp;gt; python -m timeit &lt;span class="o"&gt;[&lt;/span&gt;...&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="m"&gt;1000&lt;/span&gt; loops, best of 3: 1.11 msec per loop
&lt;/pre&gt;
&lt;p&gt;But the best speedup has a price: relaxing standard compliance with &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-Ofast&lt;/span&gt;&lt;/tt&gt;
can be beneficial if you're not using denormalized numbers, infinity and the
monstrosity that lies with &lt;tt class="docutils literal"&gt;NaN&lt;/tt&gt;:&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
&amp;gt; pythran cda.py -fvisibility&lt;span class="o"&gt;=&lt;/span&gt;hidden -fwhole-program -Wl,-strip-all -Ofast -march&lt;span class="o"&gt;=&lt;/span&gt;native
&amp;gt; python -m timeit &lt;span class="o"&gt;[&lt;/span&gt;...&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="m"&gt;1000&lt;/span&gt; loops, best of 3: 1.02 msec per loop
&lt;/pre&gt;
&lt;p&gt;If you're really into compiler flags tuning, you can try out &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-funroll-loops&lt;/span&gt;&lt;/tt&gt;
or try to tune the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-finline-limit=N&lt;/span&gt;&lt;/tt&gt; parameter (that actually get mets dow
to &lt;tt class="docutils literal"&gt;1ms per loop&lt;/tt&gt;) but that's going a bit too far :-)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="don-t-forget-vectorization"&gt;
&lt;h2&gt;Don't forget Vectorization&lt;/h2&gt;
&lt;p&gt;Combining &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-O3&lt;/span&gt;&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-march=native&lt;/span&gt;&lt;/tt&gt; triggers compiler auto-vectorization[2]_,
but that did not helped much on our case. Indeed, automatic vectorization, as
in « I am using the multimedia instruction set of my CPU » is still a difficult
task for compilers. Fortunately Pythran helps here, and passing the
not-so-experimental-anymore-but-still-not-default flag &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-DUSE_BOOST_SIMD&lt;/span&gt;&lt;/tt&gt;
triggers some hard-coded vectorization based on &lt;tt class="docutils literal"&gt;boost.simd&lt;/tt&gt; &lt;a class="footnote-reference" href="#id7" id="id3"&gt;[3]&lt;/a&gt;, and that
&lt;strong&gt;did&lt;/strong&gt; help:&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
&amp;gt; &lt;span class="c"&gt;# esod mumixam
&lt;/span&gt;&amp;gt; python -m pythran.run cda.cpp -fvisibility&lt;span class="o"&gt;=&lt;/span&gt;hidden -fwhole-program -Wl,-strip-all -Ofast -march&lt;span class="o"&gt;=&lt;/span&gt;native -funroll-loops -finline-limit&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;100000000&lt;/span&gt; -DUSE_BOOST_SIMD
&amp;gt; python -m timeit &lt;span class="o"&gt;[&lt;/span&gt;...&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="m"&gt;1000&lt;/span&gt; loops, best of 3: &lt;span class="m"&gt;462&lt;/span&gt; usec per loo
&lt;/pre&gt;
&lt;p&gt;And that's woth 63 kilobytes :-)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="concluding-remarks"&gt;
&lt;h2&gt;Concluding Remarks&lt;/h2&gt;
&lt;p&gt;Source-to-source compilers &lt;em&gt;do&lt;/em&gt; generate ugly intermediate code, and Pythran is
not an exception. One benefit though is that you can get a full control over
the &lt;em&gt;backend&lt;/em&gt; compiler, which means you can tune it to your needs. Given some
knowledge and benchmarking effort, it can get you closer to your goal without
changing the original code.&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="id4" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id1"&gt;[0]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;And I am shamelessly advertising it :-) &lt;a class="reference external" href="http://blog.quarkslab.com/clang-hardening-cheat-sheet.html"&gt;http://blog.quarkslab.com/clang-hardening-cheat-sheet.html&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id5" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id2"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;If you really want to inspect the intermediate C++ code generated by pythran use the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-E&lt;/span&gt;&lt;/tt&gt; flag and a &lt;tt class="docutils literal"&gt;cda.cpp&lt;/tt&gt; will be generated.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id6" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[2]&lt;/td&gt;&lt;td&gt;only GCC needs this, clang turns vectorisation at &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-O2&lt;/span&gt;&lt;/tt&gt;. &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-march=native&lt;/span&gt;&lt;/tt&gt; allows it to use a more recent instruction set if available.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id7" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id3"&gt;[3]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Thanks Numscale &lt;a class="reference external" href="https://www.numscale.com/boost-simd/"&gt;https://www.numscale.com/boost-simd/&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</summary></entry><entry><title>Pythran Case: Resampling</title><link href="http://serge-sans-paille.github.io/pythran-stories/pythran-case-resampling.html" rel="alternate"></link><updated>2016-03-09T00:00:00+01:00</updated><author><name>serge-sans-paille</name></author><id>tag:serge-sans-paille.github.io,2016-03-09:pythran-stories/pythran-case-resampling.html</id><summary type="html">&lt;p&gt;While hanging on &lt;a class="reference external" href="http://stackoverflow.com"&gt;Stackoverflow&lt;/a&gt; (everybody does
this, no?) I found this &lt;a class="reference external" href="http://stackoverflow.com/questions/21468170/numba-code-slower-than-pure-python"&gt;Numpy code snippet&lt;/a&gt;:&lt;/p&gt;
&lt;pre class="code python literal-block"&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;resample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;qs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rands&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;empty_like&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;qs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;lookup&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cumsum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;qs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rands&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lookup&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;results&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;When running it through &lt;a class="reference external" href="https://docs.python.org/2/library/timeit.html"&gt;timeit&lt;/a&gt;, we get:&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
% python -m timeit -s &lt;span class="s1"&gt;'import numpy as np; np.random.seed(0) ; n = 1000; xs = np.arange(n, dtype=np.float64); qs = np.array([1.0/n,]*n); rands = np.random.rand(n); from resample import resample'&lt;/span&gt; &lt;span class="s1"&gt;'resample(qs, xs, rands)'&lt;/span&gt;
&lt;span class="m"&gt;100&lt;/span&gt; loops, best of 3: 3.02 msec per loop
&lt;/pre&gt;
&lt;p&gt;The initialization code, after the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-s&lt;/span&gt;&lt;/tt&gt; switch, is run only once, and includes a call to &lt;tt class="docutils literal"&gt;np.random.seed&lt;/tt&gt; so that further comparisons hold.&lt;/p&gt;
&lt;div class="section" id="first-step-pythran"&gt;
&lt;h2&gt;First step: Pythran&lt;/h2&gt;
&lt;p&gt;What kind of optimisations could improve this code? &lt;tt class="docutils literal"&gt;np.cumsum&lt;/tt&gt;,
&lt;tt class="docutils literal"&gt;np.argmax&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;lookup &amp;gt; key&lt;/tt&gt; all are Numpy functions, so they run as native
code and there should not be much to gain there.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;But&lt;/strong&gt; if we look carefully, &lt;tt class="docutils literal"&gt;lookup &amp;gt; key&lt;/tt&gt; is building an intermediate
array, which is then passed as argument to &lt;tt class="docutils literal"&gt;np.argmax&lt;/tt&gt;. This temporary array
is not needed, as &lt;tt class="docutils literal"&gt;np.argmax&lt;/tt&gt; could work on a stream. That's a typical
shortcoming of Numpy &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Eager_evaluation"&gt;eager evaluation&lt;/a&gt;, a pedantic word to state
that expressions are evaluated when they are called, and not when their result
is needed (which is &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Lazy_evaluation"&gt;lazy evaluation&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Pythran automatically computes when an expression can be lazily evaluated,
(even when it's bound to a variable, which is not the case here). So maybe we
could get some speedup?&lt;/p&gt;
&lt;p&gt;To use Pythran, we just add a comment line that states the expected types of
the top-level function:&lt;/p&gt;
&lt;pre class="code python literal-block"&gt;
&lt;span class="c"&gt;#pythran export resample(float[], float[], float[])&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;resample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;qs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rands&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;empty_like&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;qs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;lookup&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cumsum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;qs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rands&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lookup&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;results&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;And then call the &lt;tt class="docutils literal"&gt;pythran&lt;/tt&gt; compiler:&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
% pythran resample.py
&lt;/pre&gt;
&lt;p&gt;This turns the Python file into a native extension, namely &lt;tt class="docutils literal"&gt;resample.so&lt;/tt&gt; on Linux. Running it yields a nice speedup:&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
% python -m timeit &lt;span class="s1"&gt;'import numpy as np; np.random.seed(0) ; n = 1000; xs = np.arange(n, dtype=np.float64); qs = np.array([1.0/n,]*n); rands = np.random.rand(n); from resample import resample'&lt;/span&gt; &lt;span class="s1"&gt;'resample(qs, xs, rands)'&lt;/span&gt;
&lt;span class="m"&gt;1000&lt;/span&gt; loops, best of 3: 1.23 msec per loop
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="second-step-pythran-openmp"&gt;
&lt;h2&gt;Second step: Pythran + OpenMP&lt;/h2&gt;
&lt;p&gt;But could we do better? An astute reader would note that the for loop can be
run in parallel (iterations are independent). There's a famous standard for C,
C++ and Fortran to parallelize this kind of trivial loops (and to do many non
trivial stuff also, but that's not the point here) called &lt;a class="reference external" href="http://openmp.org/"&gt;OpenMP&lt;/a&gt;. It turns out Pythran supports OpenMP :-). By adding an extra comment (that should look pretty familiar to anyone accustomed to OpenMP) on the parallel loop:&lt;/p&gt;
&lt;pre class="code python literal-block"&gt;
&lt;span class="c"&gt;#pythran export resample(float[], float[], float[])&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;resample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;qs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rands&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;empty_like&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;qs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;lookup&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cumsum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;qs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c"&gt;#omp parallel for&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rands&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lookup&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;results&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;And adding the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-fopenmp&lt;/span&gt;&lt;/tt&gt; flag to the &lt;tt class="docutils literal"&gt;pythran&lt;/tt&gt; call:&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
% pythran resample.py -fopenmp
&lt;/pre&gt;
&lt;p&gt;We get an extra speedup (only two cores there, sorry about this :-/):&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
% python -m timeit &lt;span class="s1"&gt;'import numpy as np; np.random.seed(0) ; n = 1000; xs = np.arange(n, dtype=np.float64); qs = np.array([1.0/n,]*n); rands = np.random.rand(n); from resample import resample'&lt;/span&gt; &lt;span class="s1"&gt;'resample(qs, xs, rands)'&lt;/span&gt;
&lt;span class="m"&gt;1000&lt;/span&gt; loops, best of 3: &lt;span class="m"&gt;693&lt;/span&gt; usec per loop
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="third-step-pythran-brain"&gt;
&lt;h2&gt;Third step: Pythran + Brain&lt;/h2&gt;
&lt;p&gt;Now wait… calling &lt;tt class="docutils literal"&gt;np.argmax&lt;/tt&gt; on an array of &lt;tt class="docutils literal"&gt;bool&lt;/tt&gt; is indeed a nice trick to get the index of the first value where &lt;tt class="docutils literal"&gt;lookup &amp;gt; key&lt;/tt&gt;, but it evaluates the whole array. There's no early exit, while there could be (there's only &lt;tt class="docutils literal"&gt;0&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;1&lt;/tt&gt; after all). As pointed out on the &lt;a class="reference external" href="http://stackoverflow.com/questions/21468170/numba-code-slower-than-pure-python"&gt;SO thread&lt;/a&gt;, one could write a &lt;tt class="docutils literal"&gt;np_index(array_expr)&lt;/tt&gt; function that behaves like the &lt;tt class="docutils literal"&gt;list.index&lt;/tt&gt; one:&lt;/p&gt;
&lt;pre class="code python literal-block"&gt;
&lt;span class="c"&gt;#pythran export resample(float[], float[], float[])&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;np_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;haystack&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;needle&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;haystack&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;needle&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
    &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Value not found&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;resample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;qs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rands&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;empty_like&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;qs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;lookup&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cumsum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;qs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c"&gt;#omp parallel for&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rands&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lookup&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;results&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;There's a few things to note in this implementation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;there's no &lt;tt class="docutils literal"&gt;pythran export&lt;/tt&gt; for &lt;tt class="docutils literal"&gt;np_index&lt;/tt&gt; as it's not meant to be used outside the module;&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;np_index&lt;/tt&gt; plays well with lazy evaluation: the tail of the &lt;tt class="docutils literal"&gt;lookup &amp;gt; key&lt;/tt&gt; expression is not evaluated if a non null value is found before;&lt;/li&gt;
&lt;li&gt;Pythran supports built-in exceptions ;-)&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;And a last benchmark, without OpenMP:&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
% pythran resample.py
% python -m timeit &lt;span class="s1"&gt;'import numpy as np; np.random.seed(0) ; n = 1000; xs = np.arange(n, dtype=np.float64); qs = np.array([1.0/n,]*n); rands = np.random.rand(n); from resample import resample'&lt;/span&gt; &lt;span class="s1"&gt;'resample(qs, xs, rands)'&lt;/span&gt;
&lt;span class="m"&gt;1000&lt;/span&gt; loops, best of 3: &lt;span class="m"&gt;491&lt;/span&gt; usec per loop
&lt;/pre&gt;
&lt;p&gt;And with OpenMP:&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
% pythran resample.py -fopenmp
% python -m timeit &lt;span class="s1"&gt;'import numpy as np; np.random.seed(0) ; n = 1000; xs = np.arange(n, dtype=np.float64); qs = np.array([1.0/n,]*n); rands = np.random.rand(n); from resample import resample'&lt;/span&gt; &lt;span class="s1"&gt;'resample(qs, xs, rands)'&lt;/span&gt;
&lt;span class="m"&gt;1000&lt;/span&gt; loops, best of 3: &lt;span class="m"&gt;326&lt;/span&gt; usec per loop
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="the-stack-overflow-solution"&gt;
&lt;h2&gt;The Stack Overflow Solution&lt;/h2&gt;
&lt;p&gt;For reference, the Numba solution proposed as the answer to the Stack Overflow thread is:&lt;/p&gt;
&lt;pre class="code python literal-block"&gt;
&lt;span class="nd"&gt;&amp;#64;nb.jit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;f8&lt;/span&gt;&lt;span class="p"&gt;[:](&lt;/span&gt;&lt;span class="n"&gt;nb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;f8&lt;/span&gt;&lt;span class="p"&gt;[:]))&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;numba_cumsum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cumsum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nd"&gt;&amp;#64;nb.autojit&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;numba_resample2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;qs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rands&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;qs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;lookup&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numba_cumsum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;qs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;empty&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;rands&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;lookup&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
                &lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
                &lt;span class="k"&gt;break&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;results&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;On my laptop, it runs in:&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
&lt;span class="m"&gt;10&lt;/span&gt; loops, best of 3: &lt;span class="m"&gt;419&lt;/span&gt; usec per loop
&lt;/pre&gt;
&lt;p&gt;The equivalent implementation in Pythran does not need type annotation for &lt;tt class="docutils literal"&gt;np.cumsum&lt;/tt&gt; as it's already supported:&lt;/p&gt;
&lt;pre class="code python literal-block"&gt;
&lt;span class="c"&gt;#pythran export resample(float[], float[], float[])&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;resample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;qs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rands&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;qs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;lookup&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cumsum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;qs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;empty&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c"&gt;#omp parallel for&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;rands&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;lookup&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
                &lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
                &lt;span class="k"&gt;break&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;results&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;And once compiled with Pythran it runs (no OpenMP) in:&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
&lt;span class="m"&gt;1000&lt;/span&gt; loops, best of 3: &lt;span class="m"&gt;350&lt;/span&gt; usec per loop
&lt;/pre&gt;
&lt;p&gt;Pythran and Numba timings are within the same range. Numba is still easier to
integrate (Just In Time Compilation is really nice!) but it implies lower level
implementation. Pythran can still use this implementation level efficiently,
but that's not my preferred way of programming in Python ;-).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="final-thoughts"&gt;
&lt;h2&gt;Final Thoughts&lt;/h2&gt;
&lt;p&gt;This is only a story telling of the initial Stack Overflow post, reinterpreted
with Pythran in mind. What do we learn? Numpy provides a lot of nice
facilities, but one still need to understand some of its internal to rip the
best of it. And using Pythran you can do so while keeping a relatively good
abstraction!&lt;/p&gt;
&lt;/div&gt;
</summary></entry><entry><title>Pythran 0.7.4 is out!</title><link href="http://serge-sans-paille.github.io/pythran-stories/pythran-074-is-out.html" rel="alternate"></link><updated>2016-01-07T00:00:00+01:00</updated><author><name>serge-sans-paille</name></author><id>tag:serge-sans-paille.github.io,2016-01-07:pythran-stories/pythran-074-is-out.html</id><summary type="html">&lt;p&gt;The pythran team (a great total of 2 active developers) is delighted to
announce the release of Pythran 0.7.4, available on the traditional
channels:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;pypi: &lt;a class="reference external" href="https://pypi.python.org/pypi/pythran"&gt;https://pypi.python.org/pypi/pythran&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;conda: &lt;a class="reference external" href="https://anaconda.org/serge-sans-paille/pythran"&gt;https://anaconda.org/serge-sans-paille/pythran&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;github: &lt;a class="reference external" href="https://github.com/serge-sans-paille/pythran"&gt;https://github.com/serge-sans-paille/pythran&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;As usual, here is a (new) code sample, once again adapted from a
stackoverflow question &lt;a class="footnote-reference" href="#id3" id="id1"&gt;[0]&lt;/a&gt; that showcases pythran capability:&lt;/p&gt;
&lt;pre class="code python literal-block"&gt;
&lt;span class="c"&gt;#pythran export check_mask(bool[][], bool[])&lt;/span&gt;
&lt;span class="c"&gt;#  ^~~~~~~ non intrusive top-level annotation&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="c"&gt;#      ^~~~~~ numpy support (partial)&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;check_mask&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;db&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;db&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vector&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
        &lt;span class="c"&gt;# ^~~~~ type destructuring, array view&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bitwise_and&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vector&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
        &lt;span class="c"&gt;# ^~~~~~~ optimization of high level construct&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Compiled with:&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
% pythran check_mask.py
&lt;/pre&gt;
&lt;p&gt;And benchmarked with:&lt;/p&gt;
&lt;pre class="code sh literal-block"&gt;
% python -m timeit -s &lt;span class="s1"&gt;'n=10e3 ; import numpy as np;db  = np.array(np.random.randint(2, size=(n, 4)), dtype=bool); out = np.zeros(int(n),dtype=bool); from eq import check_mask'&lt;/span&gt; &lt;span class="s1"&gt;'check_mask(db, out)'&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;On average, the CPython version runs in 137 msec while the pythran version run in 450us on my laptop :-)&lt;/p&gt;
&lt;p&gt;Here is an extract of the changelog:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;
2016-01-05 Serge Guelton &amp;lt;serge.guelton&amp;#64;telecom-bretagne.eu&amp;gt;

    * IPython's magic for pythran now supports extra compile flags

    * Pythran's C++ output is compatible with Python3 and pythran3 can compile it!

    * More syntax checks (and less template traceback)

    * Improved UI (multiline pythran exports, better setup.py...)

    * Pythonic leaning / bugfixing (this tends to be a permanent item)

    * More generic support for numpy's dtype

    * Simpler install (no more boost.python deps, nor nt2 configuration)

    * Faster compilation (no more boost.python deps, smarter pass manager)

    * Better testing (gcc + clang)
&lt;/pre&gt;
&lt;p&gt;Again, thanks a lot to Pierrick for his continuous top-quality work, and
to the OpenDreamKit &lt;a class="footnote-reference" href="#id4" id="id2"&gt;[1]&lt;/a&gt; project that funded (most of) the recent developments!&lt;/p&gt;
&lt;p&gt;Special thanks to &amp;#64;hainm, &amp;#64;nbecker, &amp;#64;pkoch, &amp;#64;fsteinmetz, &amp;#64;Suor for their
feedbacks. &lt;em&gt;You&lt;/em&gt; give us the motivation to go on!&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="id3" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id1"&gt;[0]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a class="reference external" href="http://stackoverflow.com/questions/34500913/numba-slower-for-numpy-bitwise-and-on-boolean-arrays"&gt;http://stackoverflow.com/questions/34500913/numba-slower-for-numpy-bitwise-and-on-boolean-arrays&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id4" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id2"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a class="reference external" href="http://opendreamkit.org/"&gt;http://opendreamkit.org/&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</summary></entry></feed>